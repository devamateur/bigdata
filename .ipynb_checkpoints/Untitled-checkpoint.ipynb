{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f00ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer,  LoggingHandler, losses, models, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76849f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"klue/roberta-base\"\n",
    "#model_name= 'distiluse-base-multilingual-cased-v1'  # 사전 학습된 언어 모델\n",
    "\n",
    "train_batch_size = 32 \n",
    "num_epochs = 4   # 에포크 횟수\n",
    "\n",
    "model_save_path = \"output/training_klue_sts_\" + model_name.replace(\"/\", \"-\") + \"-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "embedding_model = models.Transformer(model_name)\n",
    "#embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2d3b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooler: Embedder 에서 추출된 토큰 단위 임베딩들을 가지고 문장 임베딩을 어떻게 계산할 것인지를 결정\n",
    "# Max pooling, Mean pooling등 다양한 방법이 있음\n",
    "# 여기서는 Mean pooling 사용\n",
    "\n",
    "pooler = models.Pooling(\n",
    "    embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True,\n",
    "    pooling_mode_cls_token=False,\n",
    "    pooling_mode_max_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c745011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(modules=[embedding_model, pooler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d472657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset kor_nlu (C:\\Users\\ing06\\.cache\\huggingface\\datasets\\kor_nlu\\sts\\1.0.0\\4facbba77df60b0658056ced2052633e681a50187b9428bd5752ebd59d332ba8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daec4419d1404456996b6c0bb7b09f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 가져오기\n",
    "sts_data = load_dataset(\"kor_nlu\", \"sts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36d15469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['genre', 'filename', 'year', 'id', 'score', 'sentence1', 'sentence2'],\n",
      "        num_rows: 5703\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['genre', 'filename', 'year', 'id', 'score', 'sentence1', 'sentence2'],\n",
      "        num_rows: 1471\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['genre', 'filename', 'year', 'id', 'score', 'sentence1', 'sentence2'],\n",
      "        num_rows: 1379\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# korSTS 데이터는 train, val, test데이터로 이루어져 있다\n",
    "print(sts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "335419ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genre': 1,\n",
       " 'filename': 2,\n",
       " 'year': 6,\n",
       " 'id': 73,\n",
       " 'score': 3.200000047683716,\n",
       " 'sentence1': '남자가 기타를 치고 있다.',\n",
       " 'sentence2': '한 소년이 기타를 치고 있다.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_data[\"train\"][50]   # 두 문장, 문장 간 유사도를 가지고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b230e092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genre': 1,\n",
       " 'filename': 2,\n",
       " 'year': 6,\n",
       " 'id': 24,\n",
       " 'score': 2.5,\n",
       " 'sentence1': '한 소녀가 머리를 스타일링하고 있다.',\n",
       " 'sentence2': '한 소녀가 머리를 빗고 있다.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sts 데이터셋을 sentence-transformers 훈련 양식에 맞게 변환해주는 작업\n",
    "train_samples = []\n",
    "dev_samples = []\n",
    "test_samples = []\n",
    "\n",
    "\n",
    "# KorSTS 내 테스트 데이터 예제 변환\n",
    "for phase in [\"train\", \"validation\", \"test\"]:\n",
    "    examples = datasets[phase]\n",
    "\n",
    "    for example in examples:\n",
    "        score = float(example[\"features\"][\"score\"]) / 5.0  # 0.0 ~ 1.0 스케일로 유사도 정규화\n",
    "\n",
    "        inp_example = InputExample(\n",
    "            texts=[example[\"sentence1\"], example[\"sentence2\"]], \n",
    "            label=score,\n",
    "        )\n",
    "\n",
    "        if phase == \"train\":\n",
    "            train_samples.append(inp_example)\n",
    "        elif phase == \"validation\":\n",
    "            dev_samples.append(inp_example)\n",
    "        else:\n",
    "            test_samples.append(inp_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd97612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
