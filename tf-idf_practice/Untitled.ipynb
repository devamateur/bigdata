{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f3ad506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import Dict, Text, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd2c14d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating Dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>595</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>587</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>543</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>441</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id item_id  rating\n",
       "0       0     595       3\n",
       "1       0     587       9\n",
       "2       0     543       7\n",
       "3       1     441       6\n",
       "4       1     307       3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 필요한 데이터 준비\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# synthesize data\n",
    "NUM_USERS = 10_000\n",
    "NUM_ITEMS = 779\n",
    "user_id = np.arange(start = 0, stop = NUM_USERS)\n",
    "item_id = np.arange(start = 0, stop = NUM_ITEMS)\n",
    "np.random.seed(42)\n",
    "\n",
    "user_item_dict = defaultdict(list)\n",
    "\n",
    "for id in user_id:\n",
    "    \n",
    "    # random the number of item generation\n",
    "    # for each user, random 3 to 5 items to be rated.\n",
    "    num_rand_item = np.random.randint(low = 3, high = 5)\n",
    "\n",
    "    # random from the item_id\n",
    "    rand_items = np.random.choice(item_id, size = num_rand_item, replace = False)\n",
    "\n",
    "    # random rating for each itme_id\n",
    "    rand_rating = np.random.randint(low = 1, high = 10, size = num_rand_item)\n",
    "\n",
    "    # collect the user-item paris.\n",
    "    for uid, iid,rating in zip([id] * num_rand_item, rand_items, rand_rating):\n",
    "        user_item_dict['user_id'].append(uid)\n",
    "        user_item_dict['item_id'].append(iid)\n",
    "        user_item_dict['rating'].append(rating)\n",
    "\n",
    "# prepare dataframe\n",
    "ratings = pd.DataFrame(user_item_dict)\n",
    "print(\"Rating Dataframe\")\n",
    "ratings[['user_id','item_id']] = ratings[['user_id','item_id']].astype(str)\n",
    "display(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89168506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item Dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>상어, 기린, 호랑이, 고릴라, 낙타는 영어로?ㅣ영어 배우기ㅣ위키와 동물언어_영어ㅣ...</td>\n",
       "      <td>https://youtube.com/watch?v=HRd98hTZb-U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>절대로 부활하면 안 되는 멸종 동물!</td>\n",
       "      <td>https://youtube.com/watch?v=c20uGI5Mmvs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>심쿵 아기동물 성장기 #OfftheFence #KBS #동물의왕국 (KBS1 202...</td>\n",
       "      <td>https://youtube.com/watch?v=p_wTEHPGAGI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>본격 귀여움 참기 챌린지! 역대급 심장 폭행범 ‘꼬물이들.zip’ I TV동물농장 ...</td>\n",
       "      <td>https://youtube.com/watch?v=c9cYIGqdcvA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>귀엽고 신기한 동물들을 만나봤어요! 동물편 모음 40분 자연 학습 체험</td>\n",
       "      <td>https://youtube.com/watch?v=VotU3AUcMuk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  item_id Unnamed: 0                                              title  \\\n",
       "0       0          0  상어, 기린, 호랑이, 고릴라, 낙타는 영어로?ㅣ영어 배우기ㅣ위키와 동물언어_영어ㅣ...   \n",
       "1       1          1                               절대로 부활하면 안 되는 멸종 동물!   \n",
       "2       2          2  심쿵 아기동물 성장기 #OfftheFence #KBS #동물의왕국 (KBS1 202...   \n",
       "3       3          3  본격 귀여움 참기 챌린지! 역대급 심장 폭행범 ‘꼬물이들.zip’ I TV동물농장 ...   \n",
       "4       4          4            귀엽고 신기한 동물들을 만나봤어요! 동물편 모음 40분 자연 학습 체험   \n",
       "\n",
       "                                      link  \n",
       "0  https://youtube.com/watch?v=HRd98hTZb-U  \n",
       "1  https://youtube.com/watch?v=c20uGI5Mmvs  \n",
       "2  https://youtube.com/watch?v=p_wTEHPGAGI  \n",
       "3  https://youtube.com/watch?v=c9cYIGqdcvA  \n",
       "4  https://youtube.com/watch?v=VotU3AUcMuk  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Items 데이터\n",
    "item_dict = defaultdict(list)\n",
    "#items = pd.read_csv(\"animals_crawling.csv\")\n",
    "\n",
    "for iid in item_id:\n",
    "    item_dict['item_id'].append(iid)\n",
    "# prepare dataframe\n",
    "item_ids = pd.DataFrame(item_dict)\n",
    "items = pd.read_csv(\"animals_crawling.csv\")\n",
    "items = pd.concat([item_ids, items], axis=1)\n",
    "print(\"\\nItem Dataframe\")\n",
    "items = items.astype(str)\n",
    "display(items.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5461d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임을 딥러닝 모델에 사용하기 위해 dataset 형태로 바꿈\n",
    "def df_to_ds(df):\n",
    "\n",
    "    # convert pd.DataFrame to tf.data.Dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (dict(df[['user_id','item_id']]), df['rating']))\n",
    "    \n",
    "    # convert Tuple[Dict[Text, tf.Tensor], tf.Tensor] to Dict[Text, tf.Tensor]\n",
    "    ds = ds.map(lambda x, y: {\n",
    "    'user_id' : x['user_id'],\n",
    "    'item_id' : x['item_id'],\n",
    "    'rating' : y\n",
    "    })\n",
    "\n",
    "    return ds.batch(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b596b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(keras.Model):\n",
    "\n",
    "    def __init__(self, user_id, item_id, embedding_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # user model\n",
    "        input = keras.Input(shape=(), dtype=tf.string)\n",
    "        x = keras.layers.StringLookup(\n",
    "            vocabulary = user_id, mask_token = None\n",
    "            )(input)\n",
    "        output = keras.layers.Embedding(\n",
    "            input_dim = len(user_id) + 1,\n",
    "            output_dim = embedding_size,\n",
    "            name = 'embedding'\n",
    "        )(x)\n",
    "        self.user_model = keras.Model(inputs = input,\n",
    "                                      outputs = output,\n",
    "                                      name = 'user_model')\n",
    "\n",
    "        # item model\n",
    "        input = keras.Input(shape=(), dtype=tf.string)\n",
    "        x = keras.layers.StringLookup(\n",
    "            vocabulary = item_id, mask_token = None\n",
    "            )(input)\n",
    "        output = keras.layers.Embedding(\n",
    "            input_dim = len(item_id) + 1,\n",
    "            output_dim = embedding_size,\n",
    "            name = 'embedding'\n",
    "        )(x)\n",
    "        self.item_model = keras.Model(inputs = input,\n",
    "                                  outputs = output,\n",
    "                                  name = 'item_model')\n",
    "\n",
    "        # rating model\n",
    "        user_input = keras.Input(shape=(embedding_size,), name='user_emb')\n",
    "        item_input = keras.Input(shape=(embedding_size,), name='item_emb')\n",
    "        x = keras.layers.Concatenate(axis=1)([user_input, item_input])\n",
    "        x = keras.layers.Dense(256, activation = 'relu')(x)\n",
    "        x = keras.layers.Dense(64, activation = 'relu')(x)\n",
    "        output = keras.layers.Dense(1)(x)\n",
    "        \n",
    "        self.rating_model = keras.Model(\n",
    "            inputs = {\n",
    "                'user_id' : user_input,\n",
    "                'item_id' : item_input\n",
    "            },\n",
    "            outputs = output,\n",
    "            name = 'rating_model'\n",
    "        )\n",
    "\n",
    "    def call(self, inputs: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "\n",
    "        user_emb = self.user_model(inputs['user_id'])\n",
    "        item_emb = self.item_model(inputs['item_id'])\n",
    "\n",
    "        prediction = self.rating_model({\n",
    "            'user_id' : user_emb,\n",
    "            'item_id' : item_emb\n",
    "        })\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb0bb046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMFModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self, user_id, item_id, embedding_size):\n",
    "        super().__init__()\n",
    "        self.ranking_model = RankingModel(user_id, item_id, embedding_size)\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss = keras.losses.MeanSquaredError(),\n",
    "            metrics = [keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "    \n",
    "    def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "        \n",
    "        return self.ranking_model(\n",
    "            {\n",
    "             'user_id' : features['user_id'], \n",
    "             'item_id' : features['item_id']\n",
    "            })\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "\n",
    "        return self.task(labels = features.pop('rating'),\n",
    "                         predictions = self.ranking_model(features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d10a5197",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167.\n 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181.\n 182. 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195.\n 196. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209.\n 210. 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223.\n 224. 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237.\n 238. 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251.\n 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265.\n 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279.\n 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293.\n 294. 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307.\n 308. 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319. 320. 321.\n 322. 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335.\n 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349.\n 350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n 392. 393. 394. 395. 396. 397. 398. 399. 400. 401. 402. 403. 404. 405.\n 406. 407. 408. 409. 410. 411. 412. 413. 414. 415. 416. 417. 418. 419.\n 420. 421. 422. 423. 424. 425. 426. 427. 428. 429. 430. 431. 432. 433.\n 434. 435. 436. 437. 438. 439. 440. 441. 442. 443. 444. 445. 446. 447.\n 448. 449. 450. 451. 452. 453. 454. 455. 456. 457. 458. 459. 460. 461.\n 462. 463. 464. 465. 466. 467. 468. 469. 470. 471. 472. 473. 474. 475.\n 476. 477. 478. 479. 480. 481. 482. 483. 484. 485. 486. 487. 488. 489.\n 490. 491. 492. 493. 494. 495. 496. 497. 498. 499. 500. 501. 502. 503.\n 504. 505. 506. 507. 508. 509. 510. 511. 512. 513. 514. 515. 516. 517.\n 518. 519. 520. 521. 522. 523. 524. 525. 526. 527. 528. 529. 530. 531.\n 532. 533. 534. 535. 536. 537. 538. 539. 540. 541. 542. 543. 544. 545.\n 546. 547. 548. 549. 550. 551. 552. 553. 554. 555. 556. 557. 558. 559.\n 560. 561. 562. 563. 564. 565. 566. 567. 568. 569. 570. 571. 572. 573.\n 574. 575. 576. 577. 578. 579. 580. 581. 582. 583. 584. 585. 586. 587.\n 588. 589. 590. 591. 592. 593. 594. 595. 596. 597. 598. 599. 600. 601.\n 602. 603. 604. 605. 606. 607. 608. 609. 610. 611. 612. 613. 614. 615.\n 616. 617. 618. 619. 620. 621. 622. 623. 624. 625. 626. 627. 628. 629.\n 630. 631. 632. 633. 634. 635. 636. 637. 638. 639. 640. 641. 642. 643.\n 644. 645. 646. 647. 648. 649. 650. 651. 652. 653. 654. 655. 656. 657.\n 658. 659. 660. 661. 662. 663. 664. 665. 666. 667. 668. 669. 670. 671.\n 672. 673. 674. 675. 676. 677. 678. 679. 680. 681. 682. 683. 684. 685.\n 686. 687. 688. 689. 690. 691. 692. 693. 694. 695. 696. 697. 698. 699.\n 700. 701. 702. 703. 704. 705. 706. 707. 708. 709. 710. 711. 712. 713.\n 714. 715. 716. 717. 718. 719. 720. 721. 722. 723. 724. 725. 726. 727.\n 728. 729. 730. 731. 732. 733. 734. 735. 736. 737. 738. 739. 740. 741.\n 742. 743. 744. 745. 746. 747. 748. 749. 750. 751. 752. 753. 754. 755.\n 756. 757. 758. 759. 760. 761. 762. 763. 764. 765. 766. 767. 768. 769.\n 770. 771. 772. 773. 774. 775. 776. 777. 778.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m item_mat \u001b[38;5;241m=\u001b[39m rated_items[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#item_mat = item_mat.set_index(item_id)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 코사인 유사도 구하기 compute similarity matix\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m corr_mat \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_mat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# get top-k similar items\u001b[39;00m\n\u001b[0;32m     20\u001b[0m ind2name \u001b[38;5;241m=\u001b[39m {ind:name \u001b[38;5;28;01mfor\u001b[39;00m ind,name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(item_genre_mat\u001b[38;5;241m.\u001b[39mindex)}\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1251\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \n\u001b[0;32m   1219\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;124;03mkernel matrix : ndarray of shape (n_samples_X, n_samples_Y)\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1251\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:147\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    144\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype_float\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m X \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    157\u001b[0m         X,\n\u001b[0;32m    158\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    163\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:769\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    770\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    771\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    772\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    773\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    774\u001b[0m         )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# make sure we actually converted to numeric:\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167.\n 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181.\n 182. 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195.\n 196. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209.\n 210. 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223.\n 224. 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237.\n 238. 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251.\n 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265.\n 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279.\n 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293.\n 294. 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307.\n 308. 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319. 320. 321.\n 322. 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335.\n 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349.\n 350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n 392. 393. 394. 395. 396. 397. 398. 399. 400. 401. 402. 403. 404. 405.\n 406. 407. 408. 409. 410. 411. 412. 413. 414. 415. 416. 417. 418. 419.\n 420. 421. 422. 423. 424. 425. 426. 427. 428. 429. 430. 431. 432. 433.\n 434. 435. 436. 437. 438. 439. 440. 441. 442. 443. 444. 445. 446. 447.\n 448. 449. 450. 451. 452. 453. 454. 455. 456. 457. 458. 459. 460. 461.\n 462. 463. 464. 465. 466. 467. 468. 469. 470. 471. 472. 473. 474. 475.\n 476. 477. 478. 479. 480. 481. 482. 483. 484. 485. 486. 487. 488. 489.\n 490. 491. 492. 493. 494. 495. 496. 497. 498. 499. 500. 501. 502. 503.\n 504. 505. 506. 507. 508. 509. 510. 511. 512. 513. 514. 515. 516. 517.\n 518. 519. 520. 521. 522. 523. 524. 525. 526. 527. 528. 529. 530. 531.\n 532. 533. 534. 535. 536. 537. 538. 539. 540. 541. 542. 543. 544. 545.\n 546. 547. 548. 549. 550. 551. 552. 553. 554. 555. 556. 557. 558. 559.\n 560. 561. 562. 563. 564. 565. 566. 567. 568. 569. 570. 571. 572. 573.\n 574. 575. 576. 577. 578. 579. 580. 581. 582. 583. 584. 585. 586. 587.\n 588. 589. 590. 591. 592. 593. 594. 595. 596. 597. 598. 599. 600. 601.\n 602. 603. 604. 605. 606. 607. 608. 609. 610. 611. 612. 613. 614. 615.\n 616. 617. 618. 619. 620. 621. 622. 623. 624. 625. 626. 627. 628. 629.\n 630. 631. 632. 633. 634. 635. 636. 637. 638. 639. 640. 641. 642. 643.\n 644. 645. 646. 647. 648. 649. 650. 651. 652. 653. 654. 655. 656. 657.\n 658. 659. 660. 661. 662. 663. 664. 665. 666. 667. 668. 669. 670. 671.\n 672. 673. 674. 675. 676. 677. 678. 679. 680. 681. 682. 683. 684. 685.\n 686. 687. 688. 689. 690. 691. 692. 693. 694. 695. 696. 697. 698. 699.\n 700. 701. 702. 703. 704. 705. 706. 707. 708. 709. 710. 711. 712. 713.\n 714. 715. 716. 717. 718. 719. 720. 721. 722. 723. 724. 725. 726. 727.\n 728. 729. 730. 731. 732. 733. 734. 735. 736. 737. 738. 739. 740. 741.\n 742. 743. 744. 745. 746. 747. 748. 749. 750. 751. 752. 753. 754. 755.\n 756. 757. 758. 759. 760. 761. 762. 763. 764. 765. 766. 767. 768. 769.\n 770. 771. 772. 773. 774. 775. 776. 777. 778.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "def top_k_items(item_id, top_k, corr_mat, map_name):\n",
    "    \n",
    "    # sort correlation value ascendingly and select top_k item_id\n",
    "    top_items = corr_mat[item_id,:].argsort()[-top_k:][::-1] \n",
    "    top_items = [map_name[e] for e in top_items] \n",
    "\n",
    "    return top_items\n",
    "\n",
    "# preprocessing\n",
    "rated_items = items.loc[items['item_id'].isin(ratings['item_id'])].copy()\n",
    "\n",
    "# create item-genre matrix\n",
    "item_mat = rated_items['item_id'].copy()\n",
    "item_mat = item_mat.set_index(item_id)\n",
    "\n",
    "# 코사인 유사도 구하기 compute similarity matix\n",
    "corr_mat = cosine_similarity(item_mat)\n",
    "\n",
    "# get top-k similar items\n",
    "ind2name = {ind:name for ind,name in enumerate(item_genre_mat.index)}\n",
    "name2ind = {v:k for k,v in ind2name.items()}\n",
    "similar_items = top_k_items(name2ind['99'],\n",
    "                            top_k = 10,\n",
    "                            corr_mat = corr_mat,\n",
    "                            map_name = ind2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f56973cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation on the test set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'root_mean_squared_error': 2.573801279067993,\n",
       " 'loss': 7.641561508178711,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 7.641561508178711}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top-k similar video to item_id 99\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\\n            ...\\n            769, 770, 771, 772, 773, 774, 775, 776, 777, 778],\\n           dtype='int64', length=779)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 46>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe top-k similar video to item_id 99\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m similar_items \u001b[38;5;241m=\u001b[39m top_k_items(name2ind[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m99\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     42\u001b[0m                             top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     43\u001b[0m                             corr_mat \u001b[38;5;241m=\u001b[39m item_corr_mat,\n\u001b[0;32m     44\u001b[0m                             map_name \u001b[38;5;241m=\u001b[39m ind2name)\n\u001b[1;32m---> 46\u001b[0m display(items\u001b[38;5;241m.\u001b[39mloc[\u001b[43mitems\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misin(similar_items)])\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m item_corr_mat\n\u001b[0;32m     49\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5842\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   5841\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\\n            ...\\n            769, 770, 771, 772, 773, 774, 775, 776, 777, 778],\\n           dtype='int64', length=779)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# preprocess\n",
    "train, test = train_test_split(ratings, train_size = .8, random_state=42)\n",
    "train, test = df_to_ds(train), df_to_ds(test)\n",
    "\n",
    "# # init model\n",
    "embedding_size = 64\n",
    "model = GMFModel(user_id.astype(str),\n",
    "                 item_id.astype(str),\n",
    "                 embedding_size)\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adagrad(learning_rate = .01)\n",
    ")\n",
    "\n",
    "# # fitting the model\n",
    "model.fit(train, epochs=3, verbose=0)\n",
    "\n",
    "# evaluate with the test data\n",
    "result = model.evaluate(test, return_dict=True, verbose=0)\n",
    "print(\"\\nEvaluation on the test set:\")\n",
    "display(result)\n",
    "\n",
    "# extract item embedding\n",
    "item_emb = model.ranking_model.item_model.layers[-1].get_weights()[0]\n",
    "\n",
    "\n",
    "item_corr_mat = cosine_similarity(item_emb)\n",
    "\n",
    "\n",
    "print(\"\\nThe top-k similar video to item_id 99\")\n",
    "similar_items = top_k_items(name2ind['99'],\n",
    "                            top_k = 10,\n",
    "                            corr_mat = item_corr_mat,\n",
    "                            map_name = ind2name)\n",
    "\n",
    "display(items.loc[items[item_id].isin(similar_items)])\n",
    "\n",
    "del item_corr_mat\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a997e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
